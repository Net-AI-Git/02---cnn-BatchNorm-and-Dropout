# CNN Research: BatchNorm & Dropout Placement Study

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-completed-brightgreen.svg)

## ğŸ“‹ Description

A comprehensive deep learning research study investigating the effects of different BatchNormalization and Dropout placement strategies in Convolutional Neural Networks. This project systematically compares 6 carefully designed CNN architectures on the Fashion-MNIST dataset to answer critical questions about regularization technique ordering and placement.

The study addresses fundamental questions in deep learning architecture design:
- How does BatchNorm placement (pre vs post-activation) affect convergence?
- What is the optimal order of BatchNorm and Dropout operations?
- How do different regularization strategies impact model generalization?

## ğŸ¯ Key Research Questions

1. **BatchNorm Placement**: Comparing pre-activation vs post-activation BatchNormalization
2. **Regularization Order**: Testing BatchNorm-first vs Dropout-first strategies
3. **Convergence Analysis**: Evaluating training stability and generalization performance
4. **Parameter Efficiency**: Analyzing the trade-off between model complexity and performance

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **TensorFlow 2.x / Keras** - Deep learning framework
- **NumPy** - Numerical computations
- **Matplotlib** - Data visualization
- **Scikit-learn** - Train/validation split and metrics
- **Fashion-MNIST** - Dataset (28x28 grayscale clothing images, 10 classes)

## ğŸ“ Project Structure

```
â”œâ”€â”€ cnn_BN_DO.py                 # Main research pipeline
â”œâ”€â”€ requirements.txt             # Package dependencies
â”œâ”€â”€ Fashion_MNIST_Research_Results/
â”‚   â”œâ”€â”€ 1_Modern_Baseline_Training.png
â”‚   â”œâ”€â”€ 2_Post_Activation_BatchNorm_Training.png
â”‚   â”œâ”€â”€ 3_Pre_Activation_BatchNorm_Training.png
â”‚   â”œâ”€â”€ 4_BatchNorm_First_Regularization_Training.png
â”‚   â”œâ”€â”€ 5_Dropout_First_Regularization_Training.png
â”‚   â”œâ”€â”€ 6_Classical_Regularization_Training.png
â”‚   â””â”€â”€ Final_Test_Accuracy_Comparison.png
â””â”€â”€ README.md
```

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup
1. Clone the repository:
```bash
git clone https://github.com/Net-AI-Git/02---cnn-BatchNorm-and-Dropout.git
cd 02---cnn-BatchNorm-and-Dropout
```

2. Install required packages:
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### Running the Complete Research Study
```bash
python cnn_BN_DO.py
```

The script will automatically:
- Download and preprocess Fashion-MNIST dataset
- Train 6 different CNN architectures
- Generate individual training plots for each model
- Create a comprehensive comparison visualization
- Save all results to `Fashion_MNIST_Research_Results/` directory

### Model Architectures Tested

1. **Modern Baseline** - No regularization control group
2. **Post-Activation BatchNorm** - Conv â†’ ReLU â†’ BatchNorm (Ioffe & Szegedy, 2015)
3. **Pre-Activation BatchNorm** - Conv â†’ BatchNorm â†’ ReLU (He et al., 2016)
4. **BatchNorm-First** - Conv â†’ BatchNorm â†’ ReLU â†’ Dropout
5. **Dropout-First** - Conv â†’ ReLU â†’ Dropout â†’ BatchNorm  
6. **Classical Regularization** - Conv â†’ ReLU â†’ BatchNorm â†’ Dropout

## ğŸ“Š Results & Evaluation

### Training Visualizations
![Final_Test_Accuracy_Comparison](https://github.com/user-attachments/assets/bc94d721-a63b-4580-911d-1a0e0c55719e)
<img width="463" alt="summery" src="https://github.com/user-attachments/assets/3a02e224-3c7b-4a93-acd0-dfe94c20d6e7" />
![1_Modern_Baseline_Training](https://github.com/user-attachments/assets/274fc299-1378-4e96-ad6d-055141a901ef)
![2_Post-Activation_BatchNorm_Training](https://github.com/user-attachments/assets/a002ea6a-c2fd-4535-9aa2-461999782256)
![3_Pre-Activation_BatchNorm_Training](https://github.com/user-attachments/assets/a24f7f36-1ee0-4c6b-8fb4-1c493f8d835c)
![4_BatchNorm-First_Regularization_Training](https://github.com/user-attachments/assets/cafe20b7-b083-4ec7-a86d-7e82c0474df6)
![5_Dropout-First_Regularization_Training](https://github.com/user-attachments/assets/03666d59-ea56-4e62-b164-f691cf2f2d5e)
![6_Classical_Regularization_Training](https://github.com/user-attachments/assets/53d658c0-0aea-4be0-88d4-be9038441269)



### Key Metrics Evaluated
- **Test Accuracy** - Final model performance on unseen data
- **Training Convergence** - Speed and stability of learning
- **Generalization Gap** - Difference between training and validation performance
- **Parameter Efficiency** - Performance per trainable parameter

### Research Findings

**ğŸ† Performance Rankings:**
1. **Dropout-First Regularization: 91.79%** - Best overall performance
2. **Modern Baseline: 90.78%** - Strong baseline without regularization
3. **Post-Activation BatchNorm: 90.15%** - Classic approach with instability
4. **Classical Regularization: 88.82%** - Standard but efficient convergence
5. **BatchNorm-First Regularization: 88.38%** - Stable but lower performance
6. **Pre-Activation BatchNorm: 88.02%** - Lowest performance despite modern approach

**ğŸ” Key Insights:**

**Training Stability Analysis:**
- **Dropout-First** showed the most stable training with excellent train-validation alignment
- **Post-Activation BatchNorm** exhibited significant instability with validation loss spikes
- **Classical Regularization** achieved fastest convergence (early stopping at epoch 6)
- **Modern Baseline** demonstrated smooth learning without regularization artifacts

**Regularization Order Impact:**
- **Dropout-First strategy outperformed BatchNorm-First by 3.41 percentage points**
- Applying stochastic regularization before normalization appears to improve feature learning
- The hypothesis that "normalize first, then regularize" was disproven by empirical results

**BatchNorm Placement Effects:**
- **Post-activation placement** (90.15%) slightly outperformed **pre-activation** (88.02%)
- Pre-activation showed better stability but lower final accuracy
- Classical post-activation approach remains competitive for Fashion-MNIST

**Convergence Patterns:**
- Models with dropout showed more consistent validation performance
- BatchNorm-only models exhibited more training instability
- Early stopping was most effective with classical regularization (6 epochs vs 10-12)

## ğŸ”¬ Technical Implementation Details

### Data Preprocessing
- Stratified train/validation split (80/20) to ensure balanced class distribution
- Normalization to [0,1] range with proper reshaping for CNN input
- One-hot encoding for categorical classification

### Training Configuration
- **Optimizer**: Adam with default parameters
- **Loss Function**: Categorical crossentropy
- **Batch Size**: 128
- **Early Stopping**: Patience of 3 epochs on validation accuracy
- **Epochs**: Maximum 12 with early stopping

### Model Architecture Features
- Progressive filter increase (32â†’64â†’128â†’256)
- Global Average Pooling to reduce parameters
- Consistent architecture base across all variants
- Strategic dropout rates (0.1 to 0.4) increasing with depth

## ğŸ”® Future Work

- [ ] Extend study to larger datasets (CIFAR-10, ImageNet)
- [ ] Investigate additional normalization techniques (LayerNorm, GroupNorm)
- [ ] Add learning rate scheduling and advanced optimization strategies
- [ ] Implement statistical significance testing across multiple runs
- [ ] Explore the interaction with different activation functions
- [ ] Study the effect on deeper networks (ResNet, DenseNet architectures)

## ğŸ“ˆ Performance Benchmarks

The research establishes baseline performance metrics for Fashion-MNIST with different regularization strategies, providing a foundation for:
- Architecture design decisions in computer vision projects
- Regularization strategy selection based on dataset characteristics
- Training optimization for similar classification tasks

## ğŸ‘¨â€ğŸ’» Author

**Netanel Itzhak**
- LinkedIn: [linkedin.com/in/netanelitzhak](https://www.linkedin.com/in/netanelitzhak)
- Email: ntitz19@gmail.com
- GitHub: [Net-AI-Git](https://github.com/Net-AI-Git)

## ğŸ™ Acknowledgments

- **Ioffe & Szegedy (2015)** - Original BatchNormalization paper
- **He et al. (2016)** - Pre-activation BatchNorm research
- **Fashion-MNIST Dataset** - Zalando Research for providing the challenging alternative to MNIST
- **TensorFlow/Keras Team** - For the excellent deep learning framework

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

*This research project demonstrates systematic experimental design in deep learning, proper statistical methodology, and comprehensive result analysis - essential skills for modern AI/ML engineering roles.*
